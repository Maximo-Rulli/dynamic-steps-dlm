{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximo-Rulli/dynamic-steps-dlm/blob/main/blocks-entropy-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dUq3ouKWdrY"
      },
      "source": [
        "# Analyzing the entropy of Diffusion blocks by DLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNxQC7tWdrZ"
      },
      "source": [
        "### Essential imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZ_ZUE2BrFyF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/mmada/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#Essential imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#Repository's functions\n",
        "from MMaDA.models import MMadaModelLM\n",
        "import MMaDA.generate as gen\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer and model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "You are using a model of type llada to instantiate a model of type mmada. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing MMadaModelLM with config: MMadaConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"Gen-Verse/MMaDA-8B-Base\",\n",
            "  \"activation_type\": \"silu\",\n",
            "  \"alibi\": false,\n",
            "  \"alibi_bias_max\": 8.0,\n",
            "  \"architectures\": [\n",
            "    \"LLaDAModelLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_layer_norm\": false,\n",
            "  \"attention_layer_norm_with_affine\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Gen-Verse/MMaDA-8B-Base--configuration_llada.LLaDAConfig\",\n",
            "    \"AutoModel\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\",\n",
            "    \"AutoModelForCausalLM\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\"\n",
            "  },\n",
            "  \"bias_for_layer_norm\": false,\n",
            "  \"block_group_size\": 1,\n",
            "  \"block_type\": \"llama\",\n",
            "  \"codebook_size\": 8192,\n",
            "  \"d_model\": 4096,\n",
            "  \"embedding_dropout\": 0.0,\n",
            "  \"embedding_size\": 134656,\n",
            "  \"eos_token_id\": 126081,\n",
            "  \"flash_attention\": false,\n",
            "  \"include_bias\": false,\n",
            "  \"include_qkv_bias\": false,\n",
            "  \"init_cutoff_factor\": null,\n",
            "  \"init_device\": \"meta\",\n",
            "  \"init_fn\": \"mitchell\",\n",
            "  \"init_std\": 0.02,\n",
            "  \"input_emb_norm\": false,\n",
            "  \"layer_norm_type\": \"rms\",\n",
            "  \"layer_norm_with_affine\": true,\n",
            "  \"llm_vocab_size\": 126464,\n",
            "  \"mask_token_id\": 126336,\n",
            "  \"max_sequence_length\": 4096,\n",
            "  \"mlp_hidden_size\": 12288,\n",
            "  \"mlp_ratio\": 4,\n",
            "  \"model_type\": \"mmada\",\n",
            "  \"multi_query_attention\": null,\n",
            "  \"n_heads\": 32,\n",
            "  \"n_kv_heads\": 32,\n",
            "  \"n_layers\": 32,\n",
            "  \"new_vocab_size\": 134656,\n",
            "  \"num_new_special_tokens\": 0,\n",
            "  \"num_vq_tokens\": 256,\n",
            "  \"pad_token_id\": 126081,\n",
            "  \"precision\": \"amp_bf16\",\n",
            "  \"pretrained_model_path\": \"/data_storage/shared/pretrained_models/LLaDA-8B-Instruct\",\n",
            "  \"residual_dropout\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope\": true,\n",
            "  \"rope_full_precision\": true,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"scale_logits\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 134656,\n",
            "  \"w_clip_vit\": false,\n",
            "  \"weight_tying\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.89it/s]\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model = MMadaModelLM.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load tokenizer chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n' }}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set tokenizer helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_tokenize(prompt:str, think:bool=False, chat:bool=True) -> torch.Tensor:\n",
        "  if think:\n",
        "    prompt = \"You should first think about the reasoning process in the mind and then provide the user with the answer. The reasoning process is enclosed within <think> </think> tags, i.e. <think> reasoning process here </think> answer here\\n\" + prompt\n",
        "  m = [{\"role\": \"user\", \"content\": prompt},]\n",
        "  prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False) if chat else prompt\n",
        "  input_ids = tokenizer(text=prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")['input_ids']\n",
        "  return input_ids.detach().clone().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run inference on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observation #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "08/07/2025\n",
        "\n",
        "With more than 47 steps, and length, the answer gets considerably shorter and concise. To be researched!!!\n",
        "\n",
        "prompt: \"If I have 2 friends and 6 apples, how many apples does each one recieve?\"\n",
        "\n",
        "steps<=47:\n",
        "answer: \"Each friend receives 3 apples.\"\n",
        "\n",
        "steps>47:\n",
        "answer: \"3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10/07/2025\n",
        "\n",
        "Run model with length 12 on the apples prompt. The input is split into 4,3 and no (1) blocks, each one assigned its corresponding steps [3,3,3,2], [4,4,3], and [11] respectively. The model is not confident at all in the last block when using 3 splits, while in the other two cases it generates a confident sequence with the same amount of total steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4kZAn_t4ZVcw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Output when splitting in 4 blocks--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maxrul/dev/dynamic-steps-dlm/MMaDA/generate.py:49: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4413.)\n",
            "  token_entropy = (prob@torch.log(prob).T).item()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -0.62109375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 126081:  -0.55859375\n",
            "Entropy of word 126081:  -0.43359375\n",
            "Entropy of word 126081:  -0.000675201416015625\n",
            "Total entropy of each block tensor([-3.4805, -2.0996, -2.6221, -0.9929])\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13, 126081, 126081, 126081]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples.<|endoftext|><|endoftext|><|endoftext|>']\n",
            "\n",
            "\n",
            "--------------------Output when splitting in 3 blocks--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -0.62109375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 220:  -3.703125\n",
            "Entropy of word 198:  -1.6171875\n",
            "Entropy of word 198:  -1.3515625\n",
            "Total entropy of each block tensor([-3.8145, -2.1689, -8.8906])\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13,    220,    198,    198]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples. \\n\\n']\n",
            "\n",
            "\n",
            "--------------------Output with no splits--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -2.84375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 126081:  -0.43359375\n",
            "Entropy of word 126081:  -0.0098876953125\n",
            "Entropy of word 126081:  -0.000675201416015625\n",
            "Total entropy of each block tensor([-10.8690])\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13, 126081, 126081, 126081]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples.<|endoftext|><|endoftext|><|endoftext|>']\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(module=gen)\n",
        "input_ids = chat_tokenize(\"If I have 2 friends and 6 apples, how many apples does each one recieve?\")\n",
        "length = 12\n",
        "\n",
        "print(f\"{'-'*20}Output when splitting in 4 blocks{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[3,3,3,2], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output when splitting in 3 blocks{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[4,4,3], gen_length=length, \\\n",
        "                          block_length=length//3, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with no splits{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[11], gen_length=length, \\\n",
        "                          block_length=length//1, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11/07/2025\n",
        "\n",
        "Now a more complex prompt is given alongside the thinking prompt for the model to reason. The output length is fixed at 256 and different distributions of steps are tested keeping fixed the number of blocks (4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------------Output with 3rd block having 1/2 of steps, and 4th block having 1/4th of steps--------------------\n",
            "Total entropy of each block tensor([-37.1358, -20.2489, -35.5521,  -7.5183])\n",
            "To determine how much Weng earned, we need to calculate the number of hours she spent babysitting and then multiply her hourly rate by the number of hours.\n",
            "\n",
            "First, let's find the number of hours she spent babysitting. Since she did 50 minutes of babysitting, we need to convert this time into hours. There are 60 minutes in an hour, so 50 minutes is equal to \\(\\frac{50}{60} = \\frac{1}{12}\\) of an hour.\n",
            "\n",
            "Now, we can multiply her hourly rate by the number of hours she spent babysitting. Weng earns $12 per hour, and she spent \\(\\frac{1}{12}\\) of an hour babysitting. Therefore, her total earnings are $1.\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(module=gen)\n",
        "input_ids = chat_tokenize(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\", think=False)\n",
        "length = 256\n",
        "\n",
        "\"\"\"print(f\"\\n\\n{'-'*20}Output with uniform and maximum step distribution{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[64,64,64,64], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence', entropy_log=False)\n",
        "\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False)[0])\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with last block having 1/4th of steps{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[64,64,64,64//4], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence', entropy_log=False)\n",
        "\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False)[0])\"\"\"\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with 3rd block having 1/2 of steps, and 4th block having 1/4th of steps{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[64,64,64//2,64//4], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence', entropy_log=False)\n",
        "\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Each'],\n",
              " [' friend'],\n",
              " [' rec'],\n",
              " ['ieve'],\n",
              " ['s'],\n",
              " [' '],\n",
              " ['3'],\n",
              " [' apples'],\n",
              " ['.'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>']]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[tokenizer.batch_decode(out[:, input_ids.shape[1]+i]) for i in range(len(out[0])-input_ids.shape[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|mdm_mask|>'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "{v:k for k,v in tokenizer.vocab.items()}[126336]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n                              Humpty Dumpty sat on a wall.\\n                              Humpty Dumpty had a great fall.\\n                              All the king's horses and all the king's men\\n                              Couldn't put Humpty together again.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"\"\"\\\n",
        "                              Humpty Dumpty sat on a wall.\\\n",
        "                              Humpty Dumpty had a great fall.\\\n",
        "                              All the king's horses and all the king's men\\\n",
        "                              Couldn't put Humpty together again.\"\"\")\n",
        "tokenizer.decode(encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mmada",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
