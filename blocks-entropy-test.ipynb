{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximo-Rulli/dynamic-steps-dlm/blob/main/blocks-entropy-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dUq3ouKWdrY"
      },
      "source": [
        "# Analyzing the entropy of Diffusion blocks by DLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNxQC7tWdrZ"
      },
      "source": [
        "### Essential imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kZ_ZUE2BrFyF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'MMaDA.generate' from '/home/maxrul/dev/dynamic-steps-dlm/MMaDA/generate.py'>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Essential imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "#Repository's functions\n",
        "from MMaDA.models import MMadaModelLM\n",
        "from MMaDA.generate import generate, custom_generate\n",
        "import MMaDA.generate as gen\n",
        "import importlib\n",
        "importlib.reload(gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer and model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "You are using a model of type llada to instantiate a model of type mmada. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing MMadaModelLM with config: MMadaConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"Gen-Verse/MMaDA-8B-Base\",\n",
            "  \"activation_type\": \"silu\",\n",
            "  \"alibi\": false,\n",
            "  \"alibi_bias_max\": 8.0,\n",
            "  \"architectures\": [\n",
            "    \"LLaDAModelLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_layer_norm\": false,\n",
            "  \"attention_layer_norm_with_affine\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Gen-Verse/MMaDA-8B-Base--configuration_llada.LLaDAConfig\",\n",
            "    \"AutoModel\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\",\n",
            "    \"AutoModelForCausalLM\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\"\n",
            "  },\n",
            "  \"bias_for_layer_norm\": false,\n",
            "  \"block_group_size\": 1,\n",
            "  \"block_type\": \"llama\",\n",
            "  \"codebook_size\": 8192,\n",
            "  \"d_model\": 4096,\n",
            "  \"embedding_dropout\": 0.0,\n",
            "  \"embedding_size\": 134656,\n",
            "  \"eos_token_id\": 126081,\n",
            "  \"flash_attention\": false,\n",
            "  \"include_bias\": false,\n",
            "  \"include_qkv_bias\": false,\n",
            "  \"init_cutoff_factor\": null,\n",
            "  \"init_device\": \"meta\",\n",
            "  \"init_fn\": \"mitchell\",\n",
            "  \"init_std\": 0.02,\n",
            "  \"input_emb_norm\": false,\n",
            "  \"layer_norm_type\": \"rms\",\n",
            "  \"layer_norm_with_affine\": true,\n",
            "  \"llm_vocab_size\": 126464,\n",
            "  \"mask_token_id\": 126336,\n",
            "  \"max_sequence_length\": 4096,\n",
            "  \"mlp_hidden_size\": 12288,\n",
            "  \"mlp_ratio\": 4,\n",
            "  \"model_type\": \"mmada\",\n",
            "  \"multi_query_attention\": null,\n",
            "  \"n_heads\": 32,\n",
            "  \"n_kv_heads\": 32,\n",
            "  \"n_layers\": 32,\n",
            "  \"new_vocab_size\": 134656,\n",
            "  \"num_new_special_tokens\": 0,\n",
            "  \"num_vq_tokens\": 256,\n",
            "  \"pad_token_id\": 126081,\n",
            "  \"precision\": \"amp_bf16\",\n",
            "  \"pretrained_model_path\": \"/data_storage/shared/pretrained_models/LLaDA-8B-Instruct\",\n",
            "  \"residual_dropout\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope\": true,\n",
            "  \"rope_full_precision\": true,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"scale_logits\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 134656,\n",
            "  \"w_clip_vit\": false,\n",
            "  \"weight_tying\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.71it/s]\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model = MMadaModelLM.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load tokenizer chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n' }}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set prompt and tokenize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"If I have 2 friends and 6 apples, how many apples does each one recieve?\"\n",
        "m = [{\"role\": \"user\", \"content\": prompt},]\n",
        "prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
        "input_ids = tokenizer(text=prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")['input_ids']\n",
        "input_ids = input_ids.detach().clone().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
              "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
              "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
              "            598,  10450, 126347,    198]], device='cuda:0')"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run inference on the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'MMaDA.generate' from '/home/maxrul/dev/dynamic-steps-dlm/MMaDA/generate.py'>"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "importlib.reload(gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With more than 47 steps, and length, the answer gets considerably shorter and concise. To be researched!!!\n",
        "\n",
        "prompt: \"If I have 2 friends and 6 apples, how many apples does each one recieve?\"\n",
        "\n",
        "steps<=47:\n",
        "answer: \"Each friend receives 3 apples.\"\n",
        "\n",
        "steps>47:\n",
        "answer: \"3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "4kZAn_t4ZVcw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,  21740,    220,     18,\n",
            "          32993,     13, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
            "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
            "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081]],\n",
            "       device='cuda:0')\n",
            "['Each friend receives 3 apples.']\n"
          ]
        }
      ],
      "source": [
        "length = 32\n",
        "out = gen.custom_generate(model, input_ids, steps=length, gen_length=length, block_length=length//2, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[126080, 126346,   3840, 126347,    198,   3808,    268,   6866,    698,\n",
              "          22391,    301,    654,    268,   6866,    698,   3484,     11, 109466,\n",
              "           1227,   2537,  24036,   5223,   3361,   1750,     13, 126348, 126346,\n",
              "            598,  10450, 126347,    198,   9885, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081]], device='cuda:0')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model, input_ids, steps=128, gen_length=128, block_length=128, temperature=1, cfg_scale=0., remasking='low_confidence')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mmada",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
