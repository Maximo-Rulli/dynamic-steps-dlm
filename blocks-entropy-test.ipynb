{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximo-Rulli/dynamic-steps-dlm/blob/main/blocks-entropy-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dUq3ouKWdrY"
      },
      "source": [
        "# Analyzing the entropy of Diffusion blocks by DLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNxQC7tWdrZ"
      },
      "source": [
        "### Essential imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZ_ZUE2BrFyF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/mmada/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#Essential imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#Repository's functions\n",
        "from MMaDA.models import MMadaModelLM\n",
        "import MMaDA.generate as gen\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer and model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "You are using a model of type llada to instantiate a model of type mmada. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing MMadaModelLM with config: MMadaConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"Gen-Verse/MMaDA-8B-Base\",\n",
            "  \"activation_type\": \"silu\",\n",
            "  \"alibi\": false,\n",
            "  \"alibi_bias_max\": 8.0,\n",
            "  \"architectures\": [\n",
            "    \"LLaDAModelLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_layer_norm\": false,\n",
            "  \"attention_layer_norm_with_affine\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Gen-Verse/MMaDA-8B-Base--configuration_llada.LLaDAConfig\",\n",
            "    \"AutoModel\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\",\n",
            "    \"AutoModelForCausalLM\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\"\n",
            "  },\n",
            "  \"bias_for_layer_norm\": false,\n",
            "  \"block_group_size\": 1,\n",
            "  \"block_type\": \"llama\",\n",
            "  \"codebook_size\": 8192,\n",
            "  \"d_model\": 4096,\n",
            "  \"embedding_dropout\": 0.0,\n",
            "  \"embedding_size\": 134656,\n",
            "  \"eos_token_id\": 126081,\n",
            "  \"flash_attention\": false,\n",
            "  \"include_bias\": false,\n",
            "  \"include_qkv_bias\": false,\n",
            "  \"init_cutoff_factor\": null,\n",
            "  \"init_device\": \"meta\",\n",
            "  \"init_fn\": \"mitchell\",\n",
            "  \"init_std\": 0.02,\n",
            "  \"input_emb_norm\": false,\n",
            "  \"layer_norm_type\": \"rms\",\n",
            "  \"layer_norm_with_affine\": true,\n",
            "  \"llm_vocab_size\": 126464,\n",
            "  \"mask_token_id\": 126336,\n",
            "  \"max_sequence_length\": 4096,\n",
            "  \"mlp_hidden_size\": 12288,\n",
            "  \"mlp_ratio\": 4,\n",
            "  \"model_type\": \"mmada\",\n",
            "  \"multi_query_attention\": null,\n",
            "  \"n_heads\": 32,\n",
            "  \"n_kv_heads\": 32,\n",
            "  \"n_layers\": 32,\n",
            "  \"new_vocab_size\": 134656,\n",
            "  \"num_new_special_tokens\": 0,\n",
            "  \"num_vq_tokens\": 256,\n",
            "  \"pad_token_id\": 126081,\n",
            "  \"precision\": \"amp_bf16\",\n",
            "  \"pretrained_model_path\": \"/data_storage/shared/pretrained_models/LLaDA-8B-Instruct\",\n",
            "  \"residual_dropout\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope\": true,\n",
            "  \"rope_full_precision\": true,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"scale_logits\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 134656,\n",
            "  \"w_clip_vit\": false,\n",
            "  \"weight_tying\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.66it/s]\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model = MMadaModelLM.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load tokenizer chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n' }}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set tokenizer helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_tokenize(prompt:str, think:bool=False) -> torch.Tensor:\n",
        "  if think:\n",
        "    prompt = \"You should first think about the reasoning process in the mind and then provide the user with the answer. The reasoning process is enclosed within <think> </think> tags, i.e. <think> reasoning process here </think> answer here\\n\" + prompt\n",
        "  m = [{\"role\": \"user\", \"content\": prompt},]\n",
        "  prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
        "  input_ids = tokenizer(text=prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")['input_ids']\n",
        "  return input_ids.detach().clone().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run inference on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observation #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "08/07/2025\n",
        "\n",
        "With more than 47 steps, and length, the answer gets considerably shorter and concise. To be researched!!!\n",
        "\n",
        "prompt: \"If I have 2 friends and 6 apples, how many apples does each one recieve?\"\n",
        "\n",
        "steps<=47:\n",
        "answer: \"Each friend receives 3 apples.\"\n",
        "\n",
        "steps>47:\n",
        "answer: \"3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10/07/2025\n",
        "\n",
        "Run model with length 12 on the apples prompt. The input is split into 4,3 and no (1) blocks, each one assigned its corresponding steps [3,3,3,2], [4,4,3], and [11] respectively. The model is not confident at all in the last block when using 3 splits, while in the other two cases it generates a confident sequence with the same amount of total steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4kZAn_t4ZVcw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Output when splitting in 4 blocks--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -0.62109375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 126081:  -0.55859375\n",
            "Entropy of word 126081:  -0.43359375\n",
            "Entropy of word 126081:  -0.000675201416015625\n",
            "Total entropy of each block tensor([-3.4805, -2.0996, -2.6221, -0.9929])\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13, 126081, 126081, 126081]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples.<|endoftext|><|endoftext|><|endoftext|>']\n",
            "\n",
            "\n",
            "--------------------Output when splitting in 3 blocks--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -0.62109375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 220:  -3.703125\n",
            "Entropy of word 198:  -1.6171875\n",
            "Entropy of word 198:  -1.3515625\n",
            "Total entropy of each block tensor([-3.8145, -2.1689, -8.8906])\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13,    220,    198,    198]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples. \\n\\n']\n",
            "\n",
            "\n",
            "--------------------Output with no splits--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -2.84375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 126081:  -0.43359375\n",
            "Entropy of word 126081:  -0.0098876953125\n",
            "Entropy of word 126081:  -0.000675201416015625\n",
            "Total entropy of each block tensor([-10.8690])\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13, 126081, 126081, 126081]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples.<|endoftext|><|endoftext|><|endoftext|>']\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(module=gen)\n",
        "input_ids = chat_tokenize(\"If I have 2 friends and 6 apples, how many apples does each one recieve?\")\n",
        "length = 12\n",
        "\n",
        "print(f\"{'-'*20}Output when splitting in 4 blocks{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[3,3,3,2], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output when splitting in 3 blocks{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[4,4,3], gen_length=length, \\\n",
        "                          block_length=length//3, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with no splits{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[11], gen_length=length, \\\n",
        "                          block_length=length//1, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11/07/2025\n",
        "\n",
        "Now a more complex prompt is given alongside the thinking prompt for the model to reason. The output length is fixed at 256 and different distributions of steps are tested keeping fixed the number of blocks (4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------------Output with uniform and maximum step distribution--------------------\n",
            "Total entropy of each block tensor([-26.1300, -17.8653,  -8.3630,  -4.9664])\n",
            "<think>\n",
            "To determine how long it would take for a rock of 10 kg to reach the ground, we need to use the equation of motion for constant acceleration. The equation is:\n",
            "\n",
            "\\[ s = \\frac{1}{2}at^2 \\]\n",
            "\n",
            "where:\n",
            "- \\( s \\) is the displacement (200 m)\n",
            "- \\( a \\) is the acceleration (0 m/s²)\n",
            "- \\( t \\) is the time in seconds\n",
            "\n",
            "Since the rock was initially thrown with zero initial velocity, the acceleration \\( a \\) is 0 m/s². Plugging in the values, we get:\n",
            "\n",
            "\\[ 200 = \\frac{1}{2} \\cdot 0 \\cdot t^2 \\]\n",
            "\\[ 200 = t^2 \\]\n",
            "\\[ t = \\sqrt{200} \\]\n",
            "\\[ t = 10 \\]\n",
            "\n",
            "Therefore, it would take 10 seconds for the rock to reach the ground.\n",
            "</think>\n",
            "10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "\n",
            "--------------------Output with last block having 1/4th of steps--------------------\n",
            "Total entropy of each block tensor([-26.1300, -17.8653,  -8.3630,  -5.1159])\n",
            "<think>\n",
            "To determine how long it would take for a rock of 10 kg to reach the ground, we need to use the equation of motion for constant acceleration. The equation is:\n",
            "\n",
            "\\[ s = \\frac{1}{2}at^2 \\]\n",
            "\n",
            "where:\n",
            "- \\( s \\) is the displacement (200 m)\n",
            "- \\( a \\) is the acceleration (0 m/s²)\n",
            "- \\( t \\) is the time in seconds\n",
            "\n",
            "Since the rock was initially thrown with zero initial velocity, the acceleration \\( a \\) is 0 m/s². Plugging in the values, we get:\n",
            "\n",
            "\\[ 200 = \\frac{1}{2} \\cdot 0 \\cdot t^2 \\]\n",
            "\\[ 200 = t^2 \\]\n",
            "\\[ t = \\sqrt{200} \\]\n",
            "\\[ t = 10 \\text{ seconds} \\]\n",
            "\n",
            "Therefore, it would take 10 seconds for the rock to reach the ground.\n",
            "</think>\n",
            "10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "\n",
            "--------------------Output with 3rd block having 1/2 of steps, and 4th block having 1/4th of steps--------------------\n",
            "Total entropy of each block tensor([-26.1300, -17.8653, -12.4225,  -5.1159])\n",
            "<think>\n",
            "To determine how long it would take for a rock of 10 kg to reach the ground, we need to use the equation of motion for constant acceleration. The equation is:\n",
            "\n",
            "\\[ s = \\frac{1}{2}at^2 \\]\n",
            "\n",
            "where:\n",
            "- \\( s \\) is the displacement (200 m)\n",
            "- \\( a \\) is the acceleration (0 m/s²)\n",
            "- \\( t \\) is the time in seconds\n",
            "\n",
            "Since the rock was initially thrown with zero initial velocity, the acceleration \\( a \\) is 0 m/s². Plugging in the values, we get:\n",
            "\n",
            "\\[ 200 = \\frac{1}{2} \\cdot 0 \\cdot t^2 \\]\n",
            "\\[ 200 = t^2 \\]\n",
            "\\[ t = \\sqrt{200} \\]\n",
            "\\[ t = 10 \\text{ seconds} \\]\n",
            "\n",
            "Therefore, it would take 10 seconds for the rock to reach the ground.\n",
            "</think>\n",
            "10<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(module=gen)\n",
        "input_ids = chat_tokenize(\"How long, in seconds, would it take for a rock of 10kg to reach the ground if \\\n",
        "  it was initially thrown with zero initial speed from 200m of height?\", think=True)\n",
        "length = 256\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with uniform and maximum step distribution{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[64,64,64,64], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence', entropy_log=False)\n",
        "\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False)[0])\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with last block having 1/4th of steps{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[64,64,64,64//4], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence', entropy_log=False)\n",
        "\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False)[0])\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with 3rd block having 1/2 of steps, and 4th block having 1/4th of steps{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[64,64,64//2,64//4], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence', entropy_log=False)\n",
        "\n",
        "print(tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Each'],\n",
              " [' friend'],\n",
              " [' rec'],\n",
              " ['ieve'],\n",
              " ['s'],\n",
              " [' '],\n",
              " ['3'],\n",
              " [' apples'],\n",
              " ['.'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>']]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[tokenizer.batch_decode(out[:, input_ids.shape[1]+i]) for i in range(len(out[0])-input_ids.shape[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mmada",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
