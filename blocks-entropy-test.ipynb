{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximo-Rulli/dynamic-steps-dlm/blob/main/blocks-entropy-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dUq3ouKWdrY"
      },
      "source": [
        "# Analyzing the entropy of Diffusion blocks by DLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNxQC7tWdrZ"
      },
      "source": [
        "### Essential imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZ_ZUE2BrFyF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/mmada/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<module 'MMaDA.generate' from '/home/maxrul/dev/dynamic-steps-dlm/MMaDA/generate.py'>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Essential imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#Repository's functions\n",
        "from MMaDA.models import MMadaModelLM\n",
        "from MMaDA.generate import generate\n",
        "import MMaDA.generate as gen\n",
        "import importlib\n",
        "importlib.reload(gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer and model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "You are using a model of type llada to instantiate a model of type mmada. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing MMadaModelLM with config: MMadaConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"Gen-Verse/MMaDA-8B-Base\",\n",
            "  \"activation_type\": \"silu\",\n",
            "  \"alibi\": false,\n",
            "  \"alibi_bias_max\": 8.0,\n",
            "  \"architectures\": [\n",
            "    \"LLaDAModelLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_layer_norm\": false,\n",
            "  \"attention_layer_norm_with_affine\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"Gen-Verse/MMaDA-8B-Base--configuration_llada.LLaDAConfig\",\n",
            "    \"AutoModel\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\",\n",
            "    \"AutoModelForCausalLM\": \"Gen-Verse/MMaDA-8B-Base--modeling_llada.LLaDAModelLM\"\n",
            "  },\n",
            "  \"bias_for_layer_norm\": false,\n",
            "  \"block_group_size\": 1,\n",
            "  \"block_type\": \"llama\",\n",
            "  \"codebook_size\": 8192,\n",
            "  \"d_model\": 4096,\n",
            "  \"embedding_dropout\": 0.0,\n",
            "  \"embedding_size\": 134656,\n",
            "  \"eos_token_id\": 126081,\n",
            "  \"flash_attention\": false,\n",
            "  \"include_bias\": false,\n",
            "  \"include_qkv_bias\": false,\n",
            "  \"init_cutoff_factor\": null,\n",
            "  \"init_device\": \"meta\",\n",
            "  \"init_fn\": \"mitchell\",\n",
            "  \"init_std\": 0.02,\n",
            "  \"input_emb_norm\": false,\n",
            "  \"layer_norm_type\": \"rms\",\n",
            "  \"layer_norm_with_affine\": true,\n",
            "  \"llm_vocab_size\": 126464,\n",
            "  \"mask_token_id\": 126336,\n",
            "  \"max_sequence_length\": 4096,\n",
            "  \"mlp_hidden_size\": 12288,\n",
            "  \"mlp_ratio\": 4,\n",
            "  \"model_type\": \"mmada\",\n",
            "  \"multi_query_attention\": null,\n",
            "  \"n_heads\": 32,\n",
            "  \"n_kv_heads\": 32,\n",
            "  \"n_layers\": 32,\n",
            "  \"new_vocab_size\": 134656,\n",
            "  \"num_new_special_tokens\": 0,\n",
            "  \"num_vq_tokens\": 256,\n",
            "  \"pad_token_id\": 126081,\n",
            "  \"precision\": \"amp_bf16\",\n",
            "  \"pretrained_model_path\": \"/data_storage/shared/pretrained_models/LLaDA-8B-Instruct\",\n",
            "  \"residual_dropout\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope\": true,\n",
            "  \"rope_full_precision\": true,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"scale_logits\": false,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 134656,\n",
            "  \"w_clip_vit\": false,\n",
            "  \"weight_tying\": false\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.58it/s]\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model = MMadaModelLM.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Gen-Verse/MMaDA-8B-Base\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load tokenizer chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n' }}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set prompt and tokenize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"If I have 2 friends and 6 apples, how many apples does each one recieve?\"\n",
        "m = [{\"role\": \"user\", \"content\": prompt},]\n",
        "prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
        "input_ids = tokenizer(text=prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")['input_ids']\n",
        "input_ids = input_ids.detach().clone().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
              "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
              "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
              "            598,  10450, 126347,    198]], device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run inference on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observation #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "08/07/2025\n",
        "\n",
        "With more than 47 steps, and length, the answer gets considerably shorter and concise. To be researched!!!\n",
        "\n",
        "prompt: \"If I have 2 friends and 6 apples, how many apples does each one recieve?\"\n",
        "\n",
        "steps<=47:\n",
        "answer: \"Each friend receives 3 apples.\"\n",
        "\n",
        "steps>47:\n",
        "answer: \"3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment #1\n",
        "10/07/2025\n",
        "\n",
        "Run model with length 12 on the apples prompt. The input is split into 4,3 and no (1) blocks, each one assigned its corresponding steps [3,3,3,2], [4,4,3], and [11] respectively. The model is not confident at all in the last block when using 3 splits, while in the other two cases it generates a confident sequence with the same amount of total steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4kZAn_t4ZVcw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Output when splitting in 4 blocks--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -0.62109375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 126081:  -0.55859375\n",
            "Entropy of word 126081:  -0.43359375\n",
            "Entropy of word 126081:  -0.000675201416015625\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13, 126081, 126081, 126081]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples.<|endoftext|><|endoftext|><|endoftext|>']\n",
            "\n",
            "\n",
            "--------------------Output when splitting in 3 blocks--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -0.62109375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 220:  -3.703125\n",
            "Entropy of word 198:  -1.6171875\n",
            "Entropy of word 198:  -1.3515625\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13,    220,    198,    198]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples. \\n\\n']\n",
            "\n",
            "\n",
            "--------------------Output with no splits--------------------\n",
            "Entropy of word 11934:  -1.453125\n",
            "Entropy of word 2684:  -2.84375\n",
            "Entropy of word 1168:  -1.40625\n",
            "Entropy of word 2925:  -0.333984375\n",
            "Entropy of word 82:  -0.90234375\n",
            "Entropy of word 220:  -0.86328125\n",
            "Entropy of word 32993:  -0.326171875\n",
            "Entropy of word 18:  -0.0771484375\n",
            "Entropy of word 13:  -2.21875\n",
            "Entropy of word 126081:  -0.43359375\n",
            "Entropy of word 126081:  -0.0098876953125\n",
            "Entropy of word 126081:  -0.000675201416015625\n",
            "tensor([[126080, 126346,   3840, 126347,    198,   2531,    331,    561,    220,\n",
            "             17,   4569,    301,    220,     21,  32993,     11,   1099,   1494,\n",
            "          32993,   1543,   1671,    810,   1168,   2925,     30, 126348, 126346,\n",
            "            598,  10450, 126347,    198,  11934,   2684,   1168,   2925,     82,\n",
            "            220,     18,  32993,     13, 126081, 126081, 126081]],\n",
            "       device='cuda:0') ['Each friend recieves 3 apples.<|endoftext|><|endoftext|><|endoftext|>']\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(module=gen)\n",
        "length = 12\n",
        "\n",
        "print(f\"{'-'*20}Output when splitting in 4 blocks{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[3,3,3,2], gen_length=length, \\\n",
        "                          block_length=length//4, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output when splitting in 3 blocks{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[4,4,3], gen_length=length, \\\n",
        "                          block_length=length//3, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))\n",
        "\n",
        "print(f\"\\n\\n{'-'*20}Output with no splits{'-'*20}\")\n",
        "out = gen.custom_generate(model, input_ids, steps=[11], gen_length=length, \\\n",
        "                          block_length=length//1, temperature=0, cfg_scale=0., remasking='low_confidence')\n",
        "\n",
        "print(out, tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Each'],\n",
              " [' friend'],\n",
              " [' rec'],\n",
              " ['ieve'],\n",
              " ['s'],\n",
              " [' '],\n",
              " ['3'],\n",
              " [' apples'],\n",
              " ['.'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>'],\n",
              " ['<|endoftext|>']]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[tokenizer.batch_decode(out[:, input_ids.shape[1]+i]) for i in range(len(out[0])-input_ids.shape[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "inv_vocab = {v:k for k,v in tokenizer.vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ġapples'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inv_vocab[32993]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "126349"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[126080, 126346,   3840, 126347,    198,   3808,    268,   6866,    698,\n",
              "          22391,    301,    654,    268,   6866,    698,   3484,     11, 109466,\n",
              "           1227,   2537,  24036,   5223,   3361,   1750,     13, 126348, 126346,\n",
              "            598,  10450, 126347,    198,   9885, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081, 126081,\n",
              "         126081, 126081, 126081, 126081, 126081, 126081]], device='cuda:0')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model, input_ids, steps=128, gen_length=128, block_length=128, temperature=1, cfg_scale=0., remasking='low_confidence')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mmada",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
