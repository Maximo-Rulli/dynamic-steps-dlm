{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Java-fusion training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Essential imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZ_ZUE2BrFyF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import Tensor\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resnet block definition (key part of U-Net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_embed_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, out_channels)\n",
        "        )\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            #nn.GroupNorm(8, in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            #nn.GroupNorm(8, out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x, t, debug=False):\n",
        "        h = self.block1(x)\n",
        "        h += self.time_mlp(t)[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "        if (debug):\n",
        "            #print(\"Inp: \\n\"+str(x[0,0])) \n",
        "            print(\"Out: \\n\"+str(h[0,0]))\n",
        "            print(\"Plus 'identity: \\n\" + str((h + self.shortcut(x))[0,0]))\n",
        "        return h + self.shortcut(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Timestep embedding (not related to Noise scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    \"\"\"Sinusoidal timestep embeddings\"\"\"\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcVD0l1arq7R"
      },
      "source": [
        "### Model architecture definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eH6WCW_frtlp"
      },
      "outputs": [],
      "source": [
        "# =================== SIMPLE U-NET ===================\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, model_channels=64, out_channels=3, num_res_blocks=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_embed_dim = model_channels * 4\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, self.time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.time_embed_dim, self.time_embed_dim),\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.conv_in = nn.Conv2d(in_channels, model_channels, 3, padding=1)\n",
        "\n",
        "        # Encoder - Fixed channel progression\n",
        "        self.down1 = nn.ModuleList([\n",
        "            ResBlock(model_channels, model_channels, self.time_embed_dim),      # 64->64\n",
        "            ResBlock(model_channels, model_channels, self.time_embed_dim),      # 64->64\n",
        "            nn.Conv2d(model_channels, model_channels * 2, 3, stride=2, padding=1)  # 64->128\n",
        "        ])\n",
        "\n",
        "        self.down2 = nn.ModuleList([\n",
        "            ResBlock(model_channels * 2, model_channels * 2, self.time_embed_dim),  # 128->128\n",
        "            ResBlock(model_channels * 2, model_channels * 4, self.time_embed_dim),  # 128->256\n",
        "            nn.Conv2d(model_channels * 4, model_channels * 4, 3, stride=2, padding=1)  # 256->256\n",
        "        ])\n",
        "\n",
        "        # Middle - operates on 256 channels\n",
        "        self.middle = nn.ModuleList([\n",
        "            ResBlock(model_channels * 4, model_channels * 4, self.time_embed_dim),  # 256->256\n",
        "            ResBlock(model_channels * 4, model_channels * 4, self.time_embed_dim),  # 256->256\n",
        "        ])\n",
        "\n",
        "        # Decoder - Fixed to handle concatenations properly\n",
        "        self.up1 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(model_channels * 4, model_channels * 4, 4, stride=2, padding=1),  # 256->256, upsample\n",
        "            ResBlock(model_channels * 4 + model_channels * 4, model_channels * 2, self.time_embed_dim),  # 512->128 (concat+reduce)\n",
        "            ResBlock(model_channels * 2 + model_channels * 2, model_channels * 2, self.time_embed_dim),  # 256->128 (concat+keep)\n",
        "        ])\n",
        "\n",
        "        self.up2 = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(model_channels * 2, model_channels * 2, 4, stride=2, padding=1),  # 128->128, upsample\n",
        "            ResBlock(model_channels * 2 + model_channels, model_channels, self.time_embed_dim),      # 192->64 (concat+reduce)\n",
        "            ResBlock(model_channels + model_channels, model_channels, self.time_embed_dim),          # 128->64 (concat+reduce)\n",
        "        ])\n",
        "\n",
        "        self.conv_out = nn.Sequential(\n",
        "            #nn.GroupNorm(8, model_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(model_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps):\n",
        "        # Time embedding\n",
        "        t = self.time_embed(timestep_embedding(timesteps, dim=64))\n",
        "\n",
        "        # Encoder\n",
        "        h = self.conv_in(x)  # 3->64\n",
        "        hs = []\n",
        "\n",
        "        # Down1: 64->64->64, then downsample to 128\n",
        "        h = self.down1[0](h, t, False)  # ResBlock: 64->64\n",
        "        hs.append(h)\n",
        "        h = self.down1[1](h, t)  # ResBlock: 64->64\n",
        "        hs.append(h)\n",
        "        h = self.down1[2](h)     # Downsample: 64->128\n",
        "\n",
        "        # Down2: 128->128->256, then downsample to 256\n",
        "        h = self.down2[0](h, t)  # ResBlock: 128->128\n",
        "        hs.append(h)\n",
        "        h = self.down2[1](h, t)  # ResBlock: 128->256\n",
        "        hs.append(h)\n",
        "        h = self.down2[2](h)     # Downsample: 256->256\n",
        "\n",
        "        # Middle: 256->256->256\n",
        "        h = self.middle[0](h, t)\n",
        "        h = self.middle[1](h, t)\n",
        "\n",
        "        # Decoder - carefully match the skip connections\n",
        "        # Up1: 256 + skip connections\n",
        "        h = self.up1[0](h)  # Upsample: 256->256\n",
        "        h = torch.cat([h, hs.pop()], dim=1)  # 256+256=512\n",
        "        h = self.up1[1](h, t)  # ResBlock: 512->128\n",
        "        h = torch.cat([h, hs.pop()], dim=1)  # 128+128=256\n",
        "        h = self.up1[2](h, t)  # ResBlock: 256->128\n",
        "\n",
        "        # Up2: 128 + skip connections\n",
        "        h = self.up2[0](h)  # Upsample: 128->128\n",
        "        h = torch.cat([h, hs.pop()], dim=1)  # 128+64=192\n",
        "        h = self.up2[1](h, t)  # ResBlock: 192->64\n",
        "        #print(h[0,26])\n",
        "        h = torch.cat([h, hs.pop()], dim=1)  # 64+64=128\n",
        "        h = self.up2[2](h, t)  # ResBlock: 128->64\n",
        "\n",
        "        return self.conv_out(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DDPM framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxNzmZqUr0nb"
      },
      "source": [
        "### Noise scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q2U7P_2dry_O"
      },
      "outputs": [],
      "source": [
        "# =================== NOISE SCHEDULE ===================\n",
        "\n",
        "def linear_beta_schedule(timesteps:int, beta_start:float=0.0001, beta_end:float=0.02) -> Tensor:\n",
        "    \"\"\"Simple linear noise schedule - most reliable\"\"\"\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main DDPM class (where all comes together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================== DDPM CLASS ===================\n",
        "class SimpleDDPM:\n",
        "    def __init__(self, model:SimpleUNet, timesteps:int=2500, device:str=\"cuda\") -> None:\n",
        "        self.model = model.to(device)\n",
        "        self.timesteps = timesteps\n",
        "        self.device = device\n",
        "\n",
        "        # Noise schedule\n",
        "        self.betas = linear_beta_schedule(timesteps).to(device)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "        # Pre-compute values for sampling\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
        "\n",
        "    def q_sample(self, x_start:Tensor, t:Tensor, noise:Optional[Tensor]=None) -> Tensor:\n",
        "        \"\"\"Add noise to images (forward process)\"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None, None]\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
        "\n",
        "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "    def train_step(self, batch:Tensor) -> Tensor:\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        x_start = batch.to(self.device)\n",
        "        batch_size = x_start.shape[0]\n",
        "\n",
        "        # Sample random timesteps\n",
        "        t = torch.randint(0, self.timesteps, (batch_size,), device=self.device).long()\n",
        "\n",
        "        # Add noise\n",
        "        noise = torch.randn_like(x_start)\n",
        "        x_noisy = self.q_sample(x_start, t, noise)\n",
        "\n",
        "        # Predict noise\n",
        "        predicted_noise = self.model(x_noisy, t)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.mse_loss(predicted_noise, noise)\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, shape:tuple, debug:bool=False, \\\n",
        "        init:Tensor=Tensor([]), alphas:Tensor=Tensor([]), betas:Tensor=Tensor([]), alphas_cumprod:Tensor=Tensor([]),  noise:Tensor=Tensor([])) \\\n",
        "        -> Tensor | tuple[np.ndarray, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
        "        \"\"\"Generate samples\"\"\"\n",
        "        device = self.device\n",
        "        b = shape[0]\n",
        "        img = torch.randn(shape, device=device) if not init.numel() else init\n",
        "        ini = img.numpy()\n",
        "        noiseHist = []\n",
        "\n",
        "        for i in tqdm(reversed(range(0, self.timesteps)), desc='Sampling'):\n",
        "            t = torch.full((b,), i, device=device, dtype=torch.long)\n",
        "\n",
        "            # Predict noise\n",
        "            predicted_noise = self.model(img, t)\n",
        "\n",
        "            # Compute coefficients\n",
        "            alpha = self.alphas[i] if not alphas.numel() else alphas[i]\n",
        "            alpha_cumprod = self.alphas_cumprod[i] if not alphas_cumprod.numel() else alphas_cumprod[i]\n",
        "            beta = self.betas[i] if not betas.numel() else betas[i]\n",
        "\n",
        "            # Update image\n",
        "            img = (1 / torch.sqrt(alpha)) * (img - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n",
        "\n",
        "            if i > 0:\n",
        "                noise = torch.randn_like(img) if not noise.numel() else noise[i]\n",
        "                if debug:\n",
        "                    noiseHist.append(noise)\n",
        "                img += torch.sqrt(beta) * noise\n",
        "\n",
        "        return img if not debug else (ini, img, self.alphas, self.alphas_cumprod, self.betas, torch.stack(noiseHist, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJdDZac9sulJ"
      },
      "source": [
        "### Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BC_79EFQIfu3"
      },
      "outputs": [],
      "source": [
        "# =================== TRAINING SETUP ===================\n",
        "def get_cifar10_dataloader(batch_size=32, class_idx=6):  # 6 = frogs\n",
        "    \"\"\"Load CIFAR-10 dataset filtered to single class\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Scale to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    # Filter to only the desired class\n",
        "    indices = [i for i, (_, label) in enumerate(dataset) if label == class_idx]\n",
        "    subset = torch.utils.data.Subset(dataset, indices)\n",
        "\n",
        "    print(f\"Training on {len(indices)} samples from class {class_idx} (ships)\")\n",
        "\n",
        "    return DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "def get_mnist_dataloader(batch_size=32, digit:int=None):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Single channel normalization\n",
        "    ])\n",
        "\n",
        "    dataset = torchvision.datasets.MNIST(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    if digit:\n",
        "      # Filter to only the desired digit\n",
        "      indices = [i for i, (_, label) in enumerate(dataset) if label == digit]\n",
        "      subset = torch.utils.data.Subset(dataset, indices)\n",
        "\n",
        "      print(f\"Training on {len(indices)} samples of digit {digit}\")\n",
        "\n",
        "    if digit:\n",
        "      return DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "def show_samples(samples):\n",
        "    \"\"\"Display generated samples\"\"\"\n",
        "    samples = (samples + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
        "    samples = torch.clamp(samples, 0, 1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
        "    for i in range(4):\n",
        "        img = samples[i].cpu().permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G62uvCIGsoN-"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YptHOEwzsrlI"
      },
      "outputs": [],
      "source": [
        "def train_ddpm(epochs=10, dataset=\"cifar-10\", digit=None):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Setup\n",
        "    if dataset == \"cifar-10\":\n",
        "      model = SimpleUNet()\n",
        "      dataloader = get_cifar10_dataloader(batch_size=32, class_idx=8)  # ships\n",
        "    elif dataset == \"mnist\":\n",
        "      model = SimpleUNet(in_channels=1, out_channels=1)\n",
        "      dataloader = get_mnist_dataloader(batch_size=32, digit=None)\n",
        "\n",
        "    ddpm = SimpleDDPM(model, timesteps=2500, device=device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Lower initial LR\n",
        "\n",
        "    # Add learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "        for batch, _ in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            loss = ddpm.train_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'lr': f'{scheduler.get_last_lr()[0]:.6f}'\n",
        "            })\n",
        "\n",
        "        # Step the scheduler after each epoch\n",
        "        scheduler.step()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1} - Average Loss: {avg_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Generate samples every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model.eval()\n",
        "            if dataset==\"cifar-10\":\n",
        "              samples = ddpm.sample((4, 3, 32, 32))\n",
        "            elif dataset==\"mnist\":\n",
        "              samples = ddpm.sample((4, 1, 28, 28))\n",
        "            show_samples(samples)\n",
        "            model.train()\n",
        "\n",
        "    return ddpm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zJ2gAlLpMKho",
        "outputId": "9d3f8a72-8a35-44e1-fd35-c49b9e79acd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ddpm_mnist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mtrain_ddpm\u001b[1;34m(epochs, dataset, digit)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     10\u001b[0m   model \u001b[38;5;241m=\u001b[39m SimpleUNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m   dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_mnist_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ddpm \u001b[38;5;241m=\u001b[39m SimpleDDPM(model, timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)  \u001b[38;5;66;03m# Lower initial LR\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[13], line 22\u001b[0m, in \u001b[0;36mget_mnist_dataloader\u001b[1;34m(batch_size, digit)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mnist_dataloader\u001b[39m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, digit:\u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 22\u001b[0m     transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     23\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     24\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))  \u001b[38;5;66;03m# Single channel normalization\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     ])\n\u001b[0;32m     27\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\n\u001b[0;32m     28\u001b[0m         root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[0;32m     29\u001b[0m     )\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m digit:\n\u001b[0;32m     32\u001b[0m       \u001b[38;5;66;03m# Filter to only the desired digit\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "ddpm_mnist = train_ddpm(epochs=50, dataset=\"mnist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsMJJjNJMVok"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "DPP9ZYgPImf-",
        "outputId": "0f913c3f-4a49-44cb-eb5a-9a66c14edd84"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ddpm_mnist' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate final samples\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mddpm_mnist\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m samples \u001b[38;5;241m=\u001b[39m ddpm_mnist\u001b[38;5;241m.\u001b[39msample((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n\u001b[0;32m      4\u001b[0m show_samples(samples)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ddpm_mnist' is not defined"
          ]
        }
      ],
      "source": [
        "# Generate final samples\n",
        "ddpm_mnist.model.eval()\n",
        "samples = ddpm_mnist.sample((1, 1, 28, 28))\n",
        "show_samples(samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qofo2gMjtiHY",
        "outputId": "692d2341-3a70-405b-95db-7c61a9756497"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7706561"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(p.numel() for p in ddpm_mnist.model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving and loading!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StOcdJYPtILz"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJlFPTS_tLIK",
        "outputId": "32928b95-279a-48de-9641-55fded132bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "# Save the current trained model\n",
        "torch.save({\n",
        "    'model_state_dict': ddpm_mnist.model.state_dict(),\n",
        "    'timesteps': ddpm_mnist.timesteps,\n",
        "    'epoch': 50,  # or however many epochs you trained\n",
        "}, 'ddpm_mnist_model.pth')\n",
        "\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from ddpm_fast_mnist_model.pth\n"
          ]
        }
      ],
      "source": [
        "# Sample model loading\n",
        "def load_trained_model(filepath:str='ddpm_fast_mnist_model.pth') -> SimpleDDPM:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Create model architecture\n",
        "    model = SimpleUNet(in_channels=1, out_channels=1)\n",
        "    ddpm = SimpleDDPM(model, timesteps=5, device=device)\n",
        "    \n",
        "    # Load saved weights\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    ddpm.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    print(f\"Model loaded from {filepath}\")\n",
        "    return ddpm\n",
        "\n",
        "# Usage:\n",
        "loaded_ddpm = load_trained_model()\n",
        "#ini, samples, alpha, alpha_cum, beta, noise = loaded_ddpm.sample((1, 1, 28, 28), True)\n",
        "#show_samples(samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [1, 1, 28, 28] doesn't match the broadcast shape [4, 1, 1, 28, 28]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloaded_ddpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mini\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_cum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[23], line 75\u001b[0m, in \u001b[0;36mSimpleDDPM.sample\u001b[1;34m(self, shape, debug, init, alphas, betas, alphas_cumprod, noise)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[0;32m     74\u001b[0m             noiseHist\u001b[38;5;241m.\u001b[39mappend(noise)\n\u001b[1;32m---> 75\u001b[0m         img \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(beta) \u001b[38;5;241m*\u001b[39m noise\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug \u001b[38;5;28;01melse\u001b[39;00m (ini, img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbetas, torch\u001b[38;5;241m.\u001b[39mstack(noiseHist, \u001b[38;5;241m0\u001b[39m))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 1, 28, 28] doesn't match the broadcast shape [4, 1, 1, 28, 28]"
          ]
        }
      ],
      "source": [
        "loaded_ddpm.sample((1, 1, 28, 28), False, torch.from_numpy(ini),alpha, beta, alpha_cum,\\\n",
        "  noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"noise.npy\", noise.numpy())\n",
        "np.save(\"init.npy\", ini)\n",
        "np.save(\"out.npy\", samples.numpy())\n",
        "np.save(\"alpha.npy\", alpha.numpy())\n",
        "np.save(\"alpha_cum.npy\", alpha_cum.numpy())\n",
        "np.save(\"beta.npy\", beta.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnpicklingError",
          "evalue": "Failed to interpret file 'model_out.bin' as a pickle",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:441\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_out.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:443\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to interpret file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m as a pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mUnpicklingError\u001b[0m: Failed to interpret file 'model_out.bin' as a pickle"
          ]
        }
      ],
      "source": [
        "np.load('model_out.bin', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your model checkpoint\n",
        "checkpoint = torch.load('ddpm_fast_mnist_model.pth', map_location='cpu')\n",
        "state_dict = checkpoint.get('state_dict', checkpoint)\n",
        "\n",
        "with open('layer_mapping.txt', 'w') as f:\n",
        "    for key, tensor in state_dict['model_state_dict'].items():\n",
        "        filename = 'weights/' + key.replace('.', '_') + '.npy'\n",
        "        npy_array = tensor.cpu().numpy()\n",
        "        np.save(filename, npy_array)\n",
        "\n",
        "        shape_str = str(list(npy_array.shape))\n",
        "        f.write(f\"{key} -> {filename} (shape: {shape_str})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
